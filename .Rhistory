predict_to_tibble <- function(model) {
predict(model, re.form=NA, type="response") %>%
as_tibble() %>%
mutate(row_num = row_number(),
pred = if_else(value > 0.5, "Schizophrenia", "Control")) %>%
select(row_num, pred)
}
predict(simple_model,type="response")
result <- predict_to_tibble(simple_model) %>%
left_join(danish_dat) %>%
select(pred, Diagnosis)
result
summary(simple_model)
simple_fixed <- glm(Diagnosis ~ pitch_variability, danish_dat, family = "binomial")
summary(simple_fixed)
result <- predict_to_tibble(simple_fixed) %>%
left_join(danish_dat) %>%
select(pred, Diagnosis)
result
`?initial_split
?initial_split
´´
``
)))
))89()
()
``
?initial_split
skizo_split <- initial_split(danish_dat)
skizo_split
training(skizo_split)
skizo_test <- testing(skizo_split)
skizo_test$real_ID %in% skizo_train$real_ID
skizo_train <- training(skizo_split)
skizo_test$real_ID %in% skizo_train$real_ID
skizo_split <- initial_split(danish_dat, strata = real_ID)
?createDataPartition()
?group_vfold_cv
# Manual caret fuckery
train_ids <- createDataPartition(danish_dat$real_ID, p = 0.7)
train_ids
# Manual caret fuckery
train_ids <- createDataPartition(danish_dat$real_ID, p = 0.7) %>% as.numeric()
# Manual caret fuckery
train_ids <- createDataPartition(danish_dat$real_ID, p = 0.7)[[1]]
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <-
list(analysis = filter(danish_dat, real_ID %in% train_ids) %>% pull(row_num),
assesment = filter(danish, !real_ID %in% train_ids) %>% pull(row_num))
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <-
list(analysis = filter(danish_dat, real_ID %in% train_ids) %>% pull(row_num),
assesment = filter(danish_dat, !real_ID %in% train_ids) %>% pull(row_num))
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = filter(danish_dat, real_ID %in% train_ids) %>% pull(row_num),
assesment = filter(danish_dat, !real_ID %in% train_ids) %>% pull(row_num))
indices
danish_dat$row_num[train_ids]
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = danish_dat$row_num[train_idx],
assesment = danish_dat$row_num[!train_idx])
# Manual caret fuckery
train_idx <- createDataPartition(danish_dat$real_ID, p = 0.7)[[1]]
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = danish_dat$row_num[train_idx],
assesment = danish_dat$row_num[!train_idx])
indices
indices$assesment
# Manual caret fuckery
train_idx <- createDataPartition(danish_dat$real_ID, p = 0.7)
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = danish_dat[train_idx, row_num],
assesment = danish_dat[-train_idx, row_num])
# Manual caret fuckery
train_idx <- createDataPartition(danish_dat$real_ID, p = 0.7)[[1]]
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = danish_dat$row_num[train_idx],
assesment = danish_dat$row_num[-train_idx])
indices$assesment
split_personality <- make_splits(indices, danish_dat)
split_personality <- make_splits(indices, danish_dat)
training(split_personality)
split_personality <- make_splits(indices, select(danish_dat, -row_num))
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
split_personality <- make_splits(indices, select(danish_dat, -row_num))
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
# manual split: https://github.com/tidymodels/rsample/issues/158
indices <- list(analysis = danish_dat$row_num[train_idx],
assesment = danish_dat$row_num[-train_idx])
split_personality <- make_splits(indices, select(danish_dat, -row_num))
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
data("penguins")
View(penguins)
# Create dataset with one row (JESUS!) per customer
danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric)), sd)
# Create dataset with one row (JESUS!) per customer
danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric), sd))
# Create dataset with one row (JESUS!) per customer
danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric), mean),
across(where(is.numeric), sd))
# Create dataset with one row (JESUS!) per customer
danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric), mean))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric), mean))
initial_split(sum_dat)
split_personality <- initial_split(sum_dat)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
?sum_dat
View(sum_dat)
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(across(where(is.numeric), mean)) %>%
select(-c(row_num, Participant.x, Participant.y, Study))
split_personality <- initial_split(sum_dat)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
?vfold_cv
split_personality <- initial_split(sum_dat, strata = Diagnosis)
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), mean)) %>%
select(-c(row_num, Participant.x, Participant.y, Study))
split_personality <- initial_split(sum_dat, strata = Diagnosis)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
skizo_cv <- vfold_cv(skizo_train, repeats=4, strata = Diagnosis)
View(skizo_cv)
rm(list=ls())
gc()
rm(list=ls())
gc()
library(tidymodels)
library(tidyverse)
library(lmerTest)
library(broom.mixed)
library(e1071)
library(caret)
raw_dat <- read_csv("https://raw.githubusercontent.com/EyesOfKasparov/methods3_A3/master/finalish_data.csv")
logit2prob <- function(x) {
# https://www.wolframalpha.com/input/?i=log%28p%2F%281-p%29%29+%3D+x+isolate+p
exp(x) / (1+exp(x))
}
predict_to_tibble <- function(model) {
predict(model, re.form=NA, type="response") %>%
as_tibble() %>%
mutate(row_num = row_number(),
pred = if_else(value > 0.5, "Schizophrenia", "Control")) %>%
select(row_num, pred)
}
danish_dat <- raw_dat %>%
filter(Language == "Danish") %>%
rename(pitch_variability = freq_iqr) %>%
mutate(Diagnosis = as_factor(Diagnosis),
row_num = row_number())
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), mean)) %>%
select(-c(row_num, Participant.x, Participant.y, Study))
split_personality <- initial_split(sum_dat, strata = Diagnosis)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
skizo_cv <- vfold_cv(skizo_train, repeats=4, strata = Diagnosis)
?vfold_cv
skizo_recipe <- recipe(Diagnosis ~ ., data = danish_dat) %>%
step_normalize(all_numeric())
skizo_recipe
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet")
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(rf_model)
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
View(lr_reg_grid)
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results
lr_tune_results$.notes
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results
lr_tune_results$.notes
sapply(sum_dat, ~sum(is.na(.x)))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerballIQ, TotalIQ))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerballIQ, TotalIQ))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerbalIQ, TotalIQ))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerbalIQ, TotalIQ)) %>%
replace_na(list(SANS = 0, SAPS = 0))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric()) %>%
step_knnimpute(age)
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric()) %>%
step_knnimpute(Age)
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerbalIQ, TotalIQ, Age)) %>%
replace_na(list(SANS = 0, SAPS = 0))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerbalIQ, TotalIQ, Age)) %>%
replace_na(list(SANS = 0, SAPS = 0))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
split_personality <- initial_split(sum_dat, strata = Diagnosis)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
skizo_cv <- vfold_cv(skizo_train, repeats=1, strata = Diagnosis)
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results$.notes
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x)))) %>%
select(across(.fns = ~any(is.na(.x))))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x)))) %>%
dplyr::select(across(.fns = ~any(is.na(.x))))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
# Create dataset with one row (JESUS!) per customer
sum_dat <- danish_dat %>%
group_by(real_ID) %>%
summarise(Diagnosis = first(Diagnosis),
across(where(is.numeric), ~mean(.x, na.rm=T))) %>%
select(-c(row_num, Participant.x, Participant.y, Study, VerbalIQ, NonVerbalIQ, TotalIQ, Age)) %>%
replace_na(list(SANS = 0, SAPS = 0))
sum_dat %>%
summarise(across(.fns = ~sum(is.na(.x))))
split_personality <- initial_split(sum_dat, strata = Diagnosis)
skizo_train <- training(split_personality)
skizo_test <- testing(split_personality)
skizo_cv <- vfold_cv(skizo_train, repeats=1, strata = Diagnosis)
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results$.notes
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(1, -10, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(0.1, 10, length.out = 30))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-1, 1, length.out = 1))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
skizo_cv <- vfold_cv(skizo_train, v=2, repeats=1, strata = Diagnosis)
# Defining recipe + preprocessing
skizo_recipe <- recipe(Diagnosis ~ ., data = sum_dat) %>%
step_normalize(all_numeric())
# Defining model
lr_mod <-
logistic_reg(penalty = tune(),
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Create the workflow
lr_workflow <- workflow() %>%
add_recipe(skizo_recipe) %>%
add_model(lr_mod)
# Tune model #
# Define grid
lr_reg_grid <- tibble(penalty = 10^seq(-1, 1, length.out = 1))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results$.notes
# Defining model
lr_mod <-
logistic_reg(penalty = 1,
mixture = 1) %>% # LASSO
set_engine("glmnet") %>%
set_mode("classification")
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
grid = lr_reg_grid,
metrics = metric_set(accuracy, roc_auc))
# Actually tuning!
lr_tune_results <- lr_workflow %>%
tune_grid(resamples = skizo_cv,
metrics = metric_set(accuracy, roc_auc))
lr_tune_results$.notes
